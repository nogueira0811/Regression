---
title: "Regression Analysis - STAT 452"
author: "Stefano Nogueira"
date: "2024-05-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In their 1973 study, McDonald and Schwing introduced a dataset aimed at examining the correlation between total mortality influenced by factors such as climate, socioeconomic conditions and pollution variables across 60 US cities in 1960. 
In order to investigate this relationship, we'll conduct regression analysis with the total age-adjusted mortality from all causes as our response variable. Below are the fifteen predictor variables selected for the study:

**Variable – Description** 

X1 – Mean annual precipitation (inches)  
X2 – Mean January temperature (degrees Fahrenheit)  
X3 – Mean July temperature (degrees Fahrenheit)  
X4 – Percent of population over 65 years of age  
X5 – Population per household  
X6 – Median school years completed  
X7 – Percent of housing units that are sound  
X8 – Population per square mile  
X9 – Percent of nonwhite population  
X10 – Percent employment in white-collar jobs  
X11 – Percent of families with income under $3000  
X12 – Relative pollution potential of hydrocarbons  
X13 – Relative pollution potential of oxide of nitrogen  
X14 – Relative pollution potential of sulfur dioxide  
X15 – Percent relative humidity  
Y – Total age-adjusted mortality from all causes

## Exploratory Data Analysis

```{r include=FALSE}
data <- read.table("data1.txt", header = TRUE)
```

First, let's assess the data types in our dataframe

```{r}
str(data)
```

As well as obtaining some descriptive statistics for each of the variables
```{r}
summary(data)
```

**Exploring Quantitative Variables**  

Exploring the distributions of some interesting variables in our data, we find the following graphs:
```{r}
hist(data$X6, main = "Median school years completed", xlab = "Years Completed", ylab = "Frequency")
boxplot(data[,c("X11")], main = "Percent of families with income under $3000", xlab = "Category", ylab = "Percentage")
hist(data$X7, main = "Percent of housing units that are sound", xlab = "Percentage", ylab = "Frequency")
boxplot(data[,c("X10")], main = "Percent employment in white-collar jobs", xlab = "Category", ylab = "Employment Share")
```

Finally, let's explore the relationships between variables using a scatter plot matrix.
```{r, fig.width=12, fig.height=12}
pairs(data)

```

## Building a Regression Model

To select the best model, we'll employ stepwise selection methods. With this technique we are able to identify the most relevant independent variables for our regression model as it iteratively adds or removes predictors based on their statistical significance. 

Evaluating the following metrics we obtain: 

```{r}
raw_fit <- lm(Y ~ ., data = data)
```

```{r}
summary(raw_fit)$r.squared # R^2
summary(raw_fit)$adj.r.squared # Adjusted R^2
summary(raw_fit)$sigma^2 # MSE
AIC(raw_fit) # AIC
BIC(raw_fit) # BIC
```

We’ll utilize the AIC model criterion.

```{r}
base_fit <- lm(Y ~ 1, data = data)
library(MASS)
fit <- stepAIC(base_fit, scope=list(upper=raw_fit), direction="forward", k=2)
```

```{r}
summary(fit)
```

Now let's check for the presence of outliers. We can visualize them by examining the residuals.

```{r}
plot(fit$fitted.values, rstandard(fit), xlab = "Fitted values", ylab = "R-student Residuals", main = "R-student Residuals vs. Fitted Values")
abline(h = 0, col = "red")
```

```{r include=FALSE}
library(car)
```

And check for any of them using a Bonferroni Test.

```{r}
outlierTest(fit)
```

Therefore, by utilizing Bonferroni's corrected p-value of 0.093469, we do not reject the null hypothesis. The point is not an outlier.

## Model Adequacy Checking

In order to  ensure reliability in our analysis it's imperative that our regression model adheres to the following assumptions: 

(a) The relationship between the response and the regressors is linear, at least approximately.
(b) The error term (ε) has mean zero and constant variance (σ^2).
(c) The errors are uncorrelated.
(d) The errors are normally distributed.
(e) The regressors are linearly independent.

In order to check for (a.) we examine our previous graph of residuals. 
```{r}
plot(fit$fitted.values, rstandard(fit), xlab = "Fitted values", ylab = "R-student Residuals", main = "R-student Residuals vs. Fitted Values")
abline(h = 0, col = "red")
```

As the plot shows randomly scattered residuals around zero, it follows that the relationship between the response variable and the regressors is linear, at least approximately. 

For part (b.) we first calculate the mean of the error term. 

```{r}
mean_residuals <- mean(fit$residuals)
mean_residuals
```
Which, indeed, is very close to zero. 

And checking for non-constant variance we compute a Breusch-Pagan test.
```{r}
ncvTest(fit)
```
With a p-value of 0.36859, there is insufficient evidence to reject the null hypothesis, suggesting that the assumption of constant variance is not violated. 

Now we check for (c.) regarding uncorrelation by preforming a Durbin-Watson test. 

```{r include=FALSE}
library(lmtest)
```

```{r}
dw_test <- dwtest(fit)
print(dw_test)
```

In this case, the test does not provide sufficient evidence to reject the null hypothesis of no autocorrelation, indicating that the assumption of uncorrelated errors holds. 

In order to check for (d.) to assess whether the residuals of the regression model follow a normal distribution. We compute a Shapiro-Wilk test. 

```{r}
residuals <- residuals(fit)
shapiro.test(residuals)
```

Additionally, we can visualize this by using a QQ plot. 

```{r}
qqnorm(residuals)
qqline(residuals, col = "red")
```

Finally, in (e.), we check for multicollinearity to assess whether the regressors are linearly independent. 

```{r}
vif(fit)
```
With VIF values lesser than 5, there is a clear indication of low multicollinearity or independence between predictors. 

## Final Fitted Model

Y ~ 5.21611X9 - 16.96625X6 - 1.89343X2 + 0.22530X14 + 1.64850X1 - 2.30046X3 - 62.01704X5